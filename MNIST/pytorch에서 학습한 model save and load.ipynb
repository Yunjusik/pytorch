{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from glob import glob\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import numpy as np\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 1\n",
    "\n",
    "lr = 0.001\n",
    "momentum = 0.5\n",
    "\n",
    "batch_size = 64\n",
    "test_batch_size = 64\n",
    "\n",
    "epochs = 5\n",
    "no_cuda = False\n",
    "log_interval = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 20, 5, 1)\n",
    "        self.conv2 = nn.Conv2d(20, 50, 5, 1)\n",
    "        self.fc1 = nn.Linear(4*4*50, 500)\n",
    "        self.fc2 = nn.Linear(500, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.max_pool2d(x, 2, 2)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.max_pool2d(x, 2, 2)\n",
    "        x = x.view(-1, 4*4*50)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return F.log_softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 1000)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_paths = glob('fastcampus/dataset/mnist_png/training/*/*.png')[:1000]\n",
    "test_paths = glob('fastcampus/dataset/mnist_png/testing/*/*.png')[:1000]\n",
    "\n",
    "len(train_paths), len(test_paths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "참고: https://pytorch.org/tutorials/beginner/data_loading_tutorial.html#dataset-class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(Dataset):\n",
    "    def __init__(self, data_paths, transform=None):\n",
    "\n",
    "        self.data_paths = data_paths\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        path = self.data_paths[idx]\n",
    "        image = Image.open(path).convert(\"L\")\n",
    "        label = int(path.split('\\\\')[-2])\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(seed)\n",
    "\n",
    "use_cuda = not no_cuda and torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "\n",
    "kwargs = {'num_workers': 1, 'pin_memory': True} if use_cuda else {}\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    Dataset(train_paths, \n",
    "            transforms.Compose([\n",
    "                transforms.RandomHorizontalFlip(), \n",
    "                transforms.ToTensor(), \n",
    "                transforms.Normalize(\n",
    "                    mean=[0.406], \n",
    "                    std=[0.225])])\n",
    "           ),\n",
    "    batch_size=batch_size, \n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    Dataset(test_paths,\n",
    "           transforms.Compose([\n",
    "               transforms.ToTensor(), \n",
    "               transforms.Normalize(\n",
    "                   mean=[0.406], \n",
    "                   std=[0.225])])\n",
    "           ),\n",
    "    batch_size=batch_size, \n",
    "    shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n"
     ]
    }
   ],
   "source": [
    "for i, data in enumerate(train_loader):\n",
    "    if i == 0:\n",
    "        print(data[0].shape, data[1].shape)\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Net().to(device)\n",
    "optimizer = optim.SGD(model.parameters(), lr=lr, momentum=momentum)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (Training 까지 동일과정 반복)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/1000 (0%)]\tLoss: 2.308139\n",
      "\n",
      "Test set: Average loss: 0.5606, Accuracy: 980/1000 (98%)\n",
      "\n",
      "Train Epoch: 2 [0/1000 (0%)]\tLoss: 0.516722\n",
      "\n",
      "Test set: Average loss: 0.1423, Accuracy: 980/1000 (98%)\n",
      "\n",
      "Train Epoch: 3 [0/1000 (0%)]\tLoss: 0.069972\n",
      "\n",
      "Test set: Average loss: 0.1125, Accuracy: 980/1000 (98%)\n",
      "\n",
      "Train Epoch: 4 [0/1000 (0%)]\tLoss: 0.030837\n",
      "\n",
      "Test set: Average loss: 0.1072, Accuracy: 980/1000 (98%)\n",
      "\n",
      "Train Epoch: 5 [0/1000 (0%)]\tLoss: 0.018309\n",
      "\n",
      "Test set: Average loss: 0.1067, Accuracy: 980/1000 (98%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, epochs + 1):\n",
    "    # Train Mode\n",
    "    model.train()\n",
    "\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = F.nll_loss(output, target)  # https://pytorch.org/docs/stable/nn.html#nll-loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch_idx % log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item()))\n",
    "            \n",
    "    \n",
    "    \n",
    "    # Test mode\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += F.nll_loss(output, target, reduction='sum').item() # sum up batch loss\n",
    "            pred = output.argmax(dim=1, keepdim=True) # get the index of the max log-probability\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    \n",
    "    accuracy = 100. * correct / len(test_loader.dataset)\n",
    "    \n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "공식 홈페이지에서는 바로 다음 단계에서 배울 전체 모델을 학습하는 것보다는 이 방법을 추천  \n",
    "나중에 다른 모델에 적용하거나 수정하는 등 여러가지 이유로 용이하기 때문\n",
    "\n",
    "### 모델만 weight만 저장"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://pytorch.org/tutorials/beginner/saving_loading_models.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = 'model_weight.pt'\n",
    "\n",
    "# save_path 지정\n",
    "\n",
    "# model의 weight만 저장하기 위함.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), save_path)\n",
    "# model.state_dict를 통해 저장하고, save_path에 일치하는 파일명 \n",
    "#(model_weight.pt) 로 내 workspace 경로에 저장됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias'])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.state_dict().keys()\n",
    "\n",
    "# 각 layer 별 weight가 들어있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[-1.5675e-01, -1.2712e-01, -1.0717e-01, -1.0754e-02,  7.6381e-02],\n",
       "          [ 9.4860e-02, -9.9893e-02, -3.7203e-02, -1.5823e-01,  1.4869e-01],\n",
       "          [ 1.9968e-01,  1.9236e-01,  7.7892e-02, -4.7331e-02, -9.0103e-03],\n",
       "          [ 7.1775e-02, -1.7648e-01,  7.8558e-03, -1.6794e-01, -2.0489e-01],\n",
       "          [-8.2112e-03, -6.7429e-02, -1.3166e-01,  2.8188e-02,  3.8765e-03]]],\n",
       "\n",
       "\n",
       "        [[[ 2.8782e-02, -1.1406e-01, -1.9818e-01,  2.3067e-03, -8.3430e-02],\n",
       "          [-6.9352e-02,  1.4541e-01,  3.0975e-02,  8.9983e-02, -5.7169e-02],\n",
       "          [-1.6069e-01,  3.8244e-02, -1.3349e-01,  2.4405e-02, -1.5647e-01],\n",
       "          [ 1.4911e-01, -1.3763e-01,  1.7369e-01,  1.4208e-01,  1.5199e-01],\n",
       "          [ 1.1858e-01,  4.5595e-02, -9.3454e-03, -2.3039e-03, -7.9053e-02]]],\n",
       "\n",
       "\n",
       "        [[[-1.8555e-01,  1.8094e-01,  9.0124e-03,  9.3164e-02,  6.3902e-03],\n",
       "          [ 5.6096e-02, -1.2574e-01, -1.5837e-01,  1.5066e-01,  7.4492e-02],\n",
       "          [ 9.7355e-02, -2.0191e-01,  9.6093e-03, -1.8996e-01, -2.5088e-02],\n",
       "          [-1.2098e-02, -1.4462e-01, -2.0247e-01, -2.6021e-03,  1.7724e-01],\n",
       "          [ 1.7917e-01, -1.2549e-01, -3.9812e-02, -1.7115e-01, -1.5922e-01]]],\n",
       "\n",
       "\n",
       "        [[[-5.5575e-02,  1.9525e-01, -4.9789e-02,  1.5216e-02,  1.9172e-01],\n",
       "          [ 3.4769e-02,  1.6191e-01,  1.3535e-01,  1.9206e-01,  3.8588e-02],\n",
       "          [-1.6359e-01, -1.7352e-01, -1.0274e-01,  1.6870e-01,  1.1889e-01],\n",
       "          [ 5.5865e-02, -1.5486e-01, -1.4824e-01,  1.6500e-01, -1.8518e-02],\n",
       "          [-2.0217e-01, -2.7091e-02, -4.9078e-02,  3.2263e-03,  8.4198e-02]]],\n",
       "\n",
       "\n",
       "        [[[ 7.7233e-02, -1.5203e-01, -8.7344e-05, -9.7054e-02,  1.3681e-01],\n",
       "          [ 9.8995e-02,  1.1102e-01,  1.8503e-01,  1.0208e-01, -1.9745e-01],\n",
       "          [ 4.9340e-02, -1.3747e-01,  2.9142e-02,  6.4536e-03,  2.0254e-01],\n",
       "          [-1.3849e-01, -1.9691e-01, -5.2376e-02,  3.5575e-02,  1.5030e-01],\n",
       "          [ 1.6097e-01,  1.5383e-01,  6.3708e-02, -5.5822e-02, -4.0665e-02]]],\n",
       "\n",
       "\n",
       "        [[[-1.0957e-01,  3.8902e-02, -1.3952e-01,  6.8852e-02,  1.2982e-01],\n",
       "          [ 1.7134e-01, -1.9673e-01,  1.6385e-01, -1.2674e-01, -1.7555e-02],\n",
       "          [ 1.8493e-01,  1.0972e-01, -6.4145e-02,  1.1604e-01, -1.9738e-01],\n",
       "          [ 1.5800e-01,  1.9503e-01, -1.0575e-02,  2.9881e-02, -5.0632e-02],\n",
       "          [-1.2681e-01, -9.1239e-02, -1.5413e-02, -1.6716e-01, -1.6577e-01]]],\n",
       "\n",
       "\n",
       "        [[[ 2.0172e-02, -1.3363e-01, -1.1305e-01, -5.7075e-03, -1.0862e-01],\n",
       "          [ 1.8415e-01, -7.5558e-02, -1.1596e-01, -5.7876e-02, -1.6672e-01],\n",
       "          [ 1.0202e-01,  1.6916e-01, -1.1999e-01, -7.9019e-02,  1.8356e-01],\n",
       "          [-1.4640e-01,  5.1728e-02, -1.6718e-01, -1.8259e-01, -2.9784e-03],\n",
       "          [-1.1623e-01,  1.0601e-01, -6.5272e-02, -1.8133e-03, -1.0770e-01]]],\n",
       "\n",
       "\n",
       "        [[[ 8.3583e-02,  1.8701e-01, -2.9623e-02, -1.2459e-01, -1.9693e-01],\n",
       "          [ 1.9344e-01,  8.6241e-02,  8.3312e-02,  3.3232e-02,  9.8920e-02],\n",
       "          [ 7.2663e-02,  9.9325e-02,  1.2557e-01,  2.7881e-02,  1.7136e-01],\n",
       "          [ 1.2569e-01, -5.3988e-03, -8.9535e-02,  1.2794e-02,  1.5504e-01],\n",
       "          [ 1.2760e-01, -8.1094e-02, -4.2337e-02,  7.8811e-02,  7.4508e-02]]],\n",
       "\n",
       "\n",
       "        [[[ 1.9381e-01,  9.9705e-02,  6.4197e-02,  4.5026e-02, -1.9015e-01],\n",
       "          [-7.1937e-02,  2.1979e-02, -1.4008e-01, -1.7680e-01,  1.3367e-01],\n",
       "          [ 6.8855e-02, -1.6217e-01,  1.0722e-02,  1.1653e-01,  1.1426e-01],\n",
       "          [ 1.8282e-01, -7.9844e-02,  8.8242e-02, -1.6393e-01, -2.6334e-02],\n",
       "          [-1.2835e-01, -3.8932e-02,  1.1349e-01,  9.8631e-02, -1.0737e-01]]],\n",
       "\n",
       "\n",
       "        [[[-4.9979e-02, -4.1465e-02, -7.8506e-02, -1.1178e-01,  1.4569e-01],\n",
       "          [ 1.3773e-01,  1.6511e-01, -1.1284e-01,  1.4363e-01, -6.3090e-02],\n",
       "          [-4.8859e-02, -5.7019e-02, -1.3605e-02, -9.1370e-02,  2.3998e-02],\n",
       "          [-1.3131e-01,  7.8762e-02, -1.7190e-01, -7.6639e-02, -1.4368e-01],\n",
       "          [ 1.3335e-01,  1.3506e-01, -1.6229e-01, -1.1268e-01,  1.7578e-01]]],\n",
       "\n",
       "\n",
       "        [[[ 5.8169e-02,  1.3850e-01, -1.5206e-01,  9.2784e-02,  1.2970e-01],\n",
       "          [-4.6549e-02, -8.3783e-03,  1.4796e-01,  5.7523e-02,  5.9721e-02],\n",
       "          [ 9.4934e-04, -8.2372e-02, -1.2085e-01,  2.0183e-01, -1.4795e-02],\n",
       "          [ 1.7865e-01,  1.6549e-01, -1.0345e-01,  1.3910e-02, -5.2006e-02],\n",
       "          [-2.7154e-03,  1.3340e-01,  1.1090e-01, -1.1467e-01, -1.0100e-01]]],\n",
       "\n",
       "\n",
       "        [[[-1.4388e-01, -1.7192e-01,  1.2709e-01, -1.2432e-01,  2.6008e-03],\n",
       "          [-5.9196e-04,  3.3625e-02, -1.1586e-02,  1.0528e-01,  2.9426e-02],\n",
       "          [ 1.4435e-01,  4.4473e-02, -6.1005e-04,  3.1596e-02,  1.3692e-01],\n",
       "          [ 3.0847e-02,  1.4287e-01, -1.6159e-02,  5.1151e-03,  1.7151e-01],\n",
       "          [-1.8933e-01,  2.1065e-02,  1.8494e-01, -9.8577e-02, -3.5631e-02]]],\n",
       "\n",
       "\n",
       "        [[[-1.2591e-02,  6.6196e-02,  7.6443e-02,  4.6141e-02,  7.9253e-02],\n",
       "          [-8.6403e-02,  8.5033e-02, -6.1831e-02,  1.2705e-01, -5.8404e-02],\n",
       "          [-1.7081e-01,  1.5674e-02,  1.0631e-01, -6.5277e-03,  2.6689e-03],\n",
       "          [ 8.1787e-04,  1.0787e-01,  1.7847e-02, -1.7784e-01, -1.4259e-01],\n",
       "          [ 1.2196e-02, -1.4743e-02,  1.9286e-01, -4.6484e-02,  1.5328e-01]]],\n",
       "\n",
       "\n",
       "        [[[-1.6814e-01, -1.8980e-01, -1.8407e-02, -1.2690e-01, -5.8634e-02],\n",
       "          [ 2.2897e-02, -1.7329e-01,  1.3729e-01, -1.2188e-01,  1.5283e-01],\n",
       "          [ 6.5691e-02, -3.7437e-02, -1.7527e-01,  3.2687e-02, -1.1227e-01],\n",
       "          [ 1.7306e-01, -1.6824e-01, -5.2969e-02, -1.1043e-01, -1.2896e-01],\n",
       "          [-1.0404e-01, -1.9658e-02,  1.3812e-02,  3.4133e-02,  1.0208e-01]]],\n",
       "\n",
       "\n",
       "        [[[-8.4268e-02, -1.2566e-01, -4.1606e-02,  1.9045e-01, -2.3373e-02],\n",
       "          [-1.6975e-01,  1.5778e-01,  5.1550e-02,  3.7854e-02,  3.5623e-02],\n",
       "          [ 1.4054e-01, -8.8341e-02,  1.5044e-01,  1.9644e-01, -5.0629e-02],\n",
       "          [ 1.2808e-01,  1.1378e-01,  1.9214e-01, -4.2973e-02, -1.1808e-01],\n",
       "          [-7.3632e-02, -1.3447e-01,  9.7145e-03,  1.9565e-01,  1.9535e-01]]],\n",
       "\n",
       "\n",
       "        [[[ 1.5327e-01,  2.2096e-02,  6.5829e-03,  1.3369e-01,  1.1819e-01],\n",
       "          [-1.8596e-02, -4.8384e-02,  8.1695e-02, -6.1175e-03, -1.4202e-01],\n",
       "          [-9.4261e-02,  1.6400e-01,  1.6880e-01, -7.7656e-02, -4.4480e-02],\n",
       "          [-1.0029e-01, -6.5233e-02,  1.8324e-01, -1.7822e-01,  9.3805e-02],\n",
       "          [-1.8163e-01, -9.6652e-02, -4.1395e-02,  1.2726e-02, -3.4566e-02]]],\n",
       "\n",
       "\n",
       "        [[[-9.4525e-02, -9.4258e-02, -1.2756e-01, -1.4541e-01, -1.2990e-01],\n",
       "          [ 1.9007e-01, -9.8993e-02, -1.7210e-01,  1.3181e-01,  7.2519e-02],\n",
       "          [ 1.3670e-01, -2.0238e-01, -1.9535e-01, -1.4716e-01, -1.0987e-01],\n",
       "          [-1.3329e-03,  8.4976e-02,  1.1425e-01, -1.7576e-01,  7.7061e-02],\n",
       "          [-1.1726e-01,  4.5031e-02,  6.1642e-02,  3.0577e-02,  6.9722e-02]]],\n",
       "\n",
       "\n",
       "        [[[-9.5150e-03,  1.3371e-01,  4.3692e-02,  1.7703e-01,  4.4194e-02],\n",
       "          [-1.0503e-01,  1.2994e-01,  1.2232e-02,  1.9450e-01, -2.1489e-02],\n",
       "          [ 1.9890e-01, -1.1598e-01,  1.0065e-01, -1.4957e-01,  1.3755e-01],\n",
       "          [ 3.7023e-02,  1.1379e-01, -1.7339e-01, -2.0041e-01,  9.5987e-02],\n",
       "          [-8.1872e-02,  7.8944e-02, -1.2021e-01, -6.3621e-02, -2.2442e-04]]],\n",
       "\n",
       "\n",
       "        [[[ 1.2150e-01,  1.1349e-01, -1.8320e-02, -5.7314e-02, -4.0354e-03],\n",
       "          [ 1.0407e-01, -1.6948e-01, -6.1950e-02, -1.9393e-01,  2.8001e-02],\n",
       "          [-2.3084e-02,  1.1336e-01,  1.5901e-01, -1.7235e-01, -8.9677e-02],\n",
       "          [-3.2032e-02, -1.9477e-03, -4.6163e-02, -1.2691e-01, -7.8488e-02],\n",
       "          [-5.3230e-03,  1.6095e-01, -5.0988e-02,  1.3181e-01, -1.6247e-01]]],\n",
       "\n",
       "\n",
       "        [[[ 8.5181e-03, -1.5933e-01, -1.9268e-01,  1.4617e-02, -9.6283e-02],\n",
       "          [ 1.4720e-01,  1.5525e-01, -2.0392e-01, -1.8742e-02, -3.8261e-02],\n",
       "          [-1.1490e-01, -1.4241e-01, -8.8155e-02,  8.5117e-02,  4.4555e-02],\n",
       "          [ 1.1973e-01, -1.3946e-02,  1.0190e-01,  3.6509e-02, -4.6183e-02],\n",
       "          [-1.2654e-02, -1.8390e-01, -6.3164e-03, -6.7617e-02, -6.9023e-02]]]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.state_dict()['conv1.weight']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([20, 1, 5, 5])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.state_dict()['conv1.weight'].shape\n",
    "\n",
    "# conv1 층의 모양 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Net().to(device)\n",
    "weight_dict = torch.load(save_path)\n",
    "\n",
    "# model이 현 device에 저장된 save_path로부터 대입"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias'])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weight_dict.keys()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([20, 1, 5, 5])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weight_dict['conv1.weight'].shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net(\n",
       "  (conv1): Conv2d(1, 20, kernel_size=(5, 5), stride=(1, 1))\n",
       "  (conv2): Conv2d(20, 50, kernel_size=(5, 5), stride=(1, 1))\n",
       "  (fc1): Linear(in_features=800, out_features=500, bias=True)\n",
       "  (fc2): Linear(in_features=500, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(weight_dict)\n",
    "model.eval()\n",
    "# 위 과정을 통해 최종적으로 load 확정"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save Entire model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = 'model.pt'\n",
    "# 전체 모델을 저장하기위한 path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jusik\\Anaconda3\\envs\\pytorchO\\lib\\site-packages\\torch\\serialization.py:256: UserWarning: Couldn't retrieve source code for container of type Net. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    }
   ],
   "source": [
    "torch.save(model, save_path)\n",
    "# 모델을 통째로 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net(\n",
       "  (conv1): Conv2d(1, 20, kernel_size=(5, 5), stride=(1, 1))\n",
       "  (conv2): Conv2d(20, 50, kernel_size=(5, 5), stride=(1, 1))\n",
       "  (fc1): Linear(in_features=800, out_features=500, bias=True)\n",
       "  (fc2): Linear(in_features=500, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = torch.load(save_path)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save, Load and Resuming Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path = 'checkpoint.pt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'loss': loss\n",
    "            }, checkpoint_path)\n",
    "\n",
    "# epoch / model weight/ optimizer state / loss 등등을 한번에 저장하여 checkpoint_path에 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Net().to(device)\n",
    "optimizer = optim.SGD(model.parameters(), lr=lr, momentum=momentum)\n",
    "# optimizer 불러오기\n",
    "checkpoint = torch.load(checkpoint_path)\n",
    "# 체크포인트 load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['epoch', 'model_state_dict', 'optimizer_state_dict', 'loss'])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "epoch = checkpoint['epoch']\n",
    "loss = checkpoint['loss']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net(\n",
       "  (conv1): Conv2d(1, 20, kernel_size=(5, 5), stride=(1, 1))\n",
       "  (conv2): Conv2d(20, 50, kernel_size=(5, 5), stride=(1, 1))\n",
       "  (fc1): Linear(in_features=800, out_features=500, bias=True)\n",
       "  (fc2): Linear(in_features=500, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SGD (\n",
       "Parameter Group 0\n",
       "    dampening: 0\n",
       "    lr: 0.001\n",
       "    momentum: 0.5\n",
       "    nesterov: False\n",
       "    weight_decay: 0\n",
       ")"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0129, device='cuda:0', requires_grad=True)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net(\n",
       "  (conv1): Conv2d(1, 20, kernel_size=(5, 5), stride=(1, 1))\n",
       "  (conv2): Conv2d(20, 50, kernel_size=(5, 5), stride=(1, 1))\n",
       "  (fc1): Linear(in_features=800, out_features=500, bias=True)\n",
       "  (fc2): Linear(in_features=500, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
